---
layout: post
title: Safety in Numbers - My 18th Place Solution to Porto Seguro's Kaggle Competition
---
 
The last few months I've been working on [Porto Seguro's Safe Driver Prediction Competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction), and I'm thrilled to say that I finished in 18th place, snagging my first kaggle gold medal. This was the largest kaggle competition to date with ~5,200 teams competing, slightly more than the [Santander Customer Satisfaction Competition](https://www.kaggle.com/c/santander-customer-satisfaction). We were tasked with predicting the probability that a driver would file an insurance claim within a year of taking out a policy -- a difficult binary classification problem. In this post I'll describe the problem in more detail, walk through my overarching approach, and highlight some of the most interesting methods I used.               

![seguroLB](/images/seguro_lb.png)

(By the way, if you're wondering about my team name, it means *fear that somewhere in the world, a duck or goose is watching you*. Check out those kaggle default duck avatars).

### Problem Overview & My Strategy

Seguro provided training and test datasets of close to 600k and 900k records respectively, with 57 features. The latter were anonymized in order to protect the company's trade secrets, but we were given a bit of information about the nature of each variable. Features related to the individual policyholder, the car they drove, and the region they lived in were flagged as such. All the same, the anonymity limited the role played by feature engineering in my approach. The target variable was quite unbalanced, with only ~4% of policyholders in the training data filing claims within the year (we'll see that this imbalance plays an important role in my solution). The competition's evaluation metric was the gini coefficient, a popular measure in the insurance industry which quantifies how well-ranked predicted probabilities are relative to actual class labels. Many data scientists may be more familiar with using ROC AUC for this purpose, and it turns out there's a simple relationship between them: ```gini = 2 * AUC - 1```. Optimized filing probability rankings are critical for insurance companies, as they help them price individual policies as fairly as possible.   

Unsuprisingly, predicting car accidents based on insurance signups is not easy. Even the winning solution to this competition could not break the .65 AUC threshold. Furthermore, the dataset was messy, with large numbers of missing values for some of the most predictive features. In model cross-validation, standard deviations of gini score could be as high as .01, a remarkable level of variance in context (as a reference point, most of the top 40 rankings were decided on the 4th decimal place). These factors made me quickly realize that I'd need to go the extra mile to build a stable model, and that it'd be exceptionally easy to overfit the public leaderboard without a careful methodology.

The strategy I settled on was creating a stacked ensemble, with L2-regularized logistic regression as my meta-learner. Stacking would let me exploit the strengths of multiple different models to decrease generalization error (safety in numbers), and my choice of a conservative meta-learner would smoothly handle prediction multi-collinearities and prevent overfitting. With a final stack of 16 base models, I reached a CV gini of .2924, lowering the standard deviation to .003. The relative stability of my stack was rewarded by a test score quite close to the CV score: .2916.  

For those less familiar with stacking, the below diagram taken from [this thread](https://www.kaggle.com/getting-started/18153#post103381) neatly visualizes my procedure. 

![stack](/images/stack_diagram.png)

K splits are defined as in K-fold cross validation, with each base model trained once on each of the within-fold datasets for a total of K models. After each training run, out-of-fold predictions are generated and collected alongside test set predictions. The entire collection of out-of-fold predictions then becomes a single feature to be fed to the meta-learner, and the test set predictions are averaged to create a corresponding feature. Running this process for multiple base models, our meta-learner can learn from the training labels how to combine their outputs, and the meta-learner then runs on the test set predictions to generate the final submission output. Since the training data predictions were made out-of-fold, we're able to simulate our stacker's performance on data where the labels weren't known by the base models, as in our test set. For more on stacking, check out this excellent [stacking guide](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/).   

Stacking works most effectively when the base model predictions are relatively uncorrelated, letting the stacked model minimize their respective weaknesses instead of magnifying them. For this reason, finding a diversity of models to stack is critical. Gains in stacking are often greater with a large number of decent but diverse models than with a smaller number of hyper-tuned strong models. So I made a point of prioritizing model diversity over single model perfectionism, spending more time searching for new models to add to the stack than making optimal adjustments to my existing models. My workflow centered around getting new models up to tolerably good quality, inspecting the spearman correlations between their predictions and my previous base models' predictions to check for diversity, running cross-validation on the meta-learner, and carefully including only those new models that meaningfully improved CV.

I vetted roughly 40 different models using this procedure, settling on 16 to include in the final stack. I give a complete breakdown of the chosen models in this [kaggle post](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44579#250558). Most of the models are the usual suspects (gradient boosting with xgboost and lightgbm), but a few are less common methods chosen to add diversity such as [regularized greedy forests](https://github.com/fukatani/rgf_python) and [field-aware factorization machines](https://github.com/guestwalk/libffm). My favorite component of the ensemble was an entity-embedding neural network, which I'll expand on below. In addition to use of different models, my stack gained diversity through different choices of feature sets - particularly through use of multiple representations of categorical variables (one-hot encoding, target encoding, entity embedding features). Finally, I found an extra diversity edge by using a mix of resampling strategies to address the problem's class imbalance, which I'll discuss in the final section of this post.      

### Entity Embedding



### Resampling For Model Diversity
