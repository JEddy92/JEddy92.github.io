---
layout: post
title: Safety in Numbers - My 18th Place Solution to Porto Seguro's Kaggle Competition
---
 
The last few months I've been working on [Porto Seguro's Safe Driver Prediction Competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction), and I'm thrilled to say that I finished in 18th place, snagging my first kaggle gold medal. This was the largest kaggle competition to date with ~5,200 teams competing, slightly more than the [Santander Customer Satisfaction Competition](https://www.kaggle.com/c/santander-customer-satisfaction). We were tasked with predicting the probability that a driver would file an insurance claim within a year of taking out a policy -- a difficult binary classification problem. In this post I'll describe the problem in more detail, walk through my overarching approach, and highlight some of the most interesting methods I used.               

![seguroLB](/images/seguro_lb.png)

(By the way, if you're wondering about my team name, it means *fear that somewhere in the world, a duck or goose is watching you*. Check out those kaggle default duck avatars).

### Problem Overview & My Strategy

Seguro provided training and test datasets of close to 600k and 900k records respectively, with 57 features. The latter were anonymized in order to protect the company's trade secrets, but we were given a bit of information about the nature of each variable. Features related to the individual policyholder, the car they drove, and the region they lived in were flagged as such. All the same, the anonymity limited the role played by feature engineering in my approach. The target variable was quite unbalanced, with only ~4% of policyholders in the training data filing claims within the year (we'll see that this imbalance plays an important role in my solution). The competition's evaluation metric was the gini coefficient, a popular measure in the insurance industry which quantifies how well-ranked predicted probabilities are relative to actual class labels. Many data scientists may be more familiar with using ROC AUC for this purpose, and it turns out there's a simple relationship between them: ```gini = 2 * AUC - 1```. Optimized filing probability rankings are critical for insurance companies, as they help them price individual policies as fairly as possible.   

Unsuprisingly, predicting car accidents based on insurance signups is not easy. Even the winning solution to this competition could not break the .65 AUC threshold. Furthermore, the dataset was messy, with large numbers of missing values for some of the most predictive features. In model cross-validation, standard deviations of gini score could be as high as .01, a remarkable level of variance in context (as a reference point, most of the top 40 rankings were decided on the 4th decimal place). These factors made me quickly realize that I'd need to go the extra mile to build a stable model, and that it'd be exceptionally easy to overfit the public leaderboard without a careful methodology.

The strategy I settled on was creating a stacked ensemble, with L2-regularized logistic regression as my meta-learner. Stacking would let me exploit the strengths of multiple different models to decrease generalization error, and my choice of a conservative meta-learner would smoothly handle prediction multi-collinearities and prevent overfitting. With a final stack of 16 base models, I reached a CV gini of .2924, lowering the standard deviation to .003. The relative stability of my stack was rewarded by a test score quite close to the CV score: .2916.  

For those less familiar with stacking, the below diagram taken from [this thread](https://www.kaggle.com/getting-started/18153#post103381) neatly visualizes my procedure. 

![stack](/images/stack_diagram.png)

Once I had all a pipeline for extracting all my nice word features, I got to work building and cross-validating my linear model. Since I was working with many sparse features (topic tag categorical variables + tfidfs), a basic model was highly prone to instability. We're talking an R^2 of ~.50 on the training data, but an absurd R^2 of <-90 million on the test data. This is exactly the sort of problem that regularization exists to solve, so I put the ridge regression penalty to work and tamed my unruly model coefficients. My final test R^2 was around .48, which I was very happy with given the complexity of the problem. Below is an actual vs predicted plot of my model. 

![plot1](/images/Pairplot_im.png)

Having some modest predictive accuracy was cool, but seeing the results on word feature importance was my favorite part of the project. What I found was that most of the words most strongly associated with increased viewership were related to understanding -- words like "understand", "interpret", "explain", "read", and "wikipedia". On the other hand, some of the most negative associations included "want","like", and "problem", making me think of people asking questions to get a quick fix solution to a homework problem. My takeaway from this is that if you want your SE questions to get a lot of views, you should try being kind of highbrow, intellectual, and expansive instead of being demanding and localized in your phrasing. Be like Lucille Bluth! (well, maybe not...) 

![lucille](/images/lucille.gif)

As a quick reflection, I'm really happy with how my workflow panned out for this project. It was a lot of work, but I felt that I had a handle on it the entire time and didn't try to set unreasonable expectations for what I'd be able to do (fighting off the perfectionism). When I finished a bit early, I focused on pulling together a polished presentation instead of trying to go overboard with modeling. There are definitely a bunch of things I'd want to try to build on a model like this -- trying out optimized random forests, voting methods with individual models broken out by feature type, and even fancy text feature analysis like building a convNet. But that can wait for another day.

![SE2](/images/RegressMethodology.png)

No, in my case it's not. But I'll get there! 

If you're interested in seeing the code for the scraper and analysis along with my slide presentation, check out this [repo](https://github.com/JEddy92/Metis-Project2-StatsStackExchange).
