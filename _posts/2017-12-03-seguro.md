---
layout: post
title: Safety in Numbers - My 18th Place Solution to Porto Seguro's Kaggle Competition
---
 
The last few months I've been working on [Porto Seguro's Safe Driver Prediction Competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction), and I'm thrilled to say that I finished in 18th place, snagging my first kaggle gold medal. This was the largest kaggle competition to date with ~5,200 teams competing, slightly more than the [Santander Customer Satisfaction Competition](https://www.kaggle.com/c/santander-customer-satisfaction). We were tasked with predicting the probability that a driver would file an insurance claim within a year of taking out a policy -- a difficult binary classification problem. In this post I'll describe the problem in more detail, walk through my overarching approach, and highlight some of the most interesting methods I used.               

![seguroLB](/images/seguro_lb.png)

(By the way, if you're wondering about my team name, it means *fear that somewhere in the world, a duck or goose is watching you*. Check out those kaggle default duck avatars).

### Problem Overview & My Strategy

Seguro provided training and test datasets of close to 600k and 900k records respectively, with 57 features. The latter were anonymized in order to protect the company's trade secrets, but we were given a bit of information about the nature of each variable. Features related to the individual policyholder, the car they drove, and the region they lived in were flagged as such. All the same, the anonymity limited the role played by feature engineering in my approach. The target variable was quite unbalanced, with only ~4% of policyholders in the training data filing claims within the year (we'll see that this imbalance plays an important role in my solution). The competition's evaluation metric was the gini coefficient, a popular measure in the insurance industry which quantifies how well-ranked predicted probabilities are relative to actual class labels. Many data scientists may be more familiar with using ROC AUC for this purpose, and it turns out there's a simple relationship between them: ```gini = 2 * AUC - 1```. Optimized filing probability rankings are critical for insurance companies, as they help them price individual policies as fairly as possible.   

Unsuprisingly, predicting car accidents based on insurance signups is not easy. Even the winning solution to this competition could not break the .65 AUC threshold. Furthermore, the dataset was messy, with large numbers of missing values for some of the most predictive features. In model cross-validation, standard deviations of gini score could be as high as .01, a remarkable level of variance in context (as a reference point, most of the top 40 rankings were decided on the 4th decimal place). These factors made me quickly realize that I'd need to go the extra mile to build a stable model, and that it'd be exceptionally easy to overfit the public leaderboard without a careful methodology.

The strategy I settled on was creating a stacked ensemble, with L2-regularized logistic regression as my meta-learner. Stacking would let me exploit the strengths of multiple different models to decrease generalization error (safety in numbers), and my choice of a conservative meta-learner would smoothly handle prediction multi-collinearities and prevent overfitting. With a final stack of 16 base models, I reached a CV gini of .2924, lowering the standard deviation to .003. The relative stability of my stack was rewarded by a test score quite close to the CV score: .2916.  

For those less familiar with stacking, the below diagram taken from [this thread](https://www.kaggle.com/getting-started/18153#post103381) neatly visualizes my procedure. 

![stack](/images/stack_diagram.png)

K splits are defined as in K-fold cross validation, with each base model trained once on each of the within-fold datasets for a total of K models. After each training run, out-of-fold predictions are generated and collected alongside test set predictions. The entire collection of out-of-fold predictions then becomes a single feature to be fed to the meta-learner, and the test set predictions are averaged to create a corresponding feature. Running this process for multiple base models, our meta-learner can learn from the training labels how to combine their outputs, and the meta-learner then runs on the test set predictions to generate the final submission output. Since the training data predictions were made out-of-fold, we're able to simulate our stacker's performance on data where the labels weren't known by the base models, as in our test set. For more on stacking, check out this excellent [stacking guide](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/).   

Stacking works most effectively when the base model predictions are relatively uncorrelated, letting the stacked model minimize their respective weaknesses instead of magnifying them. For this reason, finding a diversity of models to stack is critical. Gains in stacking are often greater with a large number of decent but diverse models than with a smaller number of hyper-tuned strong models. So I made a point of prioritizing model diversity over single model perfectionism, spending more time searching for new models to add to the stack than making optimal adjustments to my existing models. My workflow centered around getting new models up to tolerably good quality, inspecting the spearman correlations between their predictions and my previous base models' predictions to check for diversity, running cross-validation on the meta-learner, and carefully including only those new models that meaningfully improved CV.

I vetted roughly 40 different models using this procedure, settling on 16 to include in the final stack. I give a complete breakdown of the chosen models in this [kaggle post](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44579#250558). Most of the models are the usual suspects (gradient boosting with xgboost and lightgbm), but a few are less common methods chosen to add diversity such as [regularized greedy forests](https://github.com/fukatani/rgf_python) and [field-aware factorization machines](https://github.com/guestwalk/libffm). My favorite component of the ensemble was an entity-embedding neural network, which I'll expand on below. In addition to use of different models, my stack gained diversity through different choices of feature sets - particularly through use of multiple representations of categorical variables (one-hot encoding, target encoding, entity embedding features). Finally, I found an extra diversity edge that secured my high ranking by using a mix of resampling strategies to address the problem's class imbalance, which I'll discuss in the final section of this post.      

### Entity Embedding

At first glance, this dataset seems ill-suited for use of neural networks -- it's a relatively small, tabular dataset with many missing values and a messy, low-signal predictive task. The competition winner blew this assumption out of the water with a [groundbreaking neural network approach](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629) that centered around denoising auto-encoders, and other top scorers (see [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44558) and [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44601)) also came up with brilliant NN formulations to include in their ensembles. I didn't reach their level of sophistication, but I did build a NN that made a key contribution to my stack.

My network was based around an exciting new method for handling high-cardinality categorical variables called [entity embedding](https://arxiv.org/pdf/1604.06737.pdf). This method was well-suited to Seguro's data, where category counts ran as high as 104 (likely the model of the car). My code for this apporach is available [here](https://www.kaggle.com/aquatic/entity-embedding-neural-net). The core idea behind entity embedding is similar to word embeddings -- the goal is to learn a continuous representation of latent qualities that allows us to capture the real meaning of the feature space with reduced dimensionality. With any luck, a representation like this will be handled much better by NN neurons than the naive one-hot-encoding representation with many sparse columns. The chart below from the EE paper gives some intuition behind what EE accomplishes: it shows a 2-dimensional EE representation learned from German states as categories. The learned representation is notably similar to actual German geography!        

<center><img src="/images/Rossman2.png" width="500"></center>

Formally, EE network architectures add embedding layers for each of the categorical input features -- these are fully connected layers that take one-hot-encoded vectors as inputs and have one neuron for each embedding dimension (this dimension is chosen as a hyper-parameter). These embedding layers are then all merged together along with the continuous and binary features, flowing forward into a standard neural network structure. This means that the embeddings are learned via back-propagation, and we can consequently think of EE as a method for supervised dimensionality reduction. In fact, a very neat aspect of the learned embeddings is that they can be reused as features for other models -- one of the models in my stack was a lightgbm run on top of entity embedding categorical features, and it provided some diversity gain relative to other boosted models.

I think the broad takeaway from the NN work done on this competition is that with the right level of preparation and structural design, NNs can be competitive with and even outperform the boosting models whose dominance is usually taken for granted on challenging tabular data. Michael Jahrer's winning solution felt like a paradigm shift for tabular state of the art. I got stuck in a bit of a local optimum with my EE approach, but I have little doubt that I wouldn't have performed as well without it. That said, my final competitive edge came from going back to the basics to revisit something at the heart of the problem: class imbalance.     

### Resampling For Model Diversity

There's some irony in reaching this part last, as I always tell my students that class balance should be the first thing they think about when starting a classification problem. It was clear to me early on that some basic resampling techniques like duplicating the minority class observations (upsampling) within training folds could improve models a bit, but I didn't realize the full potential of resampling until later. Once I expanded my search for variety beyond different models and features to different resampling techniques, I discovered an untapped source of diversity that pushed my stack over the edge from the top 100 into the top 20.

There are [many methods](https://www.quora.com/Can-deep-learning-handle-imbalanced-data/answer/Joseph-Eddy) for handling class imbalance. When optimizing for AUC, these methods are used to discourage a model from treating the minority class observations as outliers and ignoring causal signal in the training data. One natural way to accomplish this is by giving extra weight to minority class observations when computing a model's log loss. Since the minority class then contributes more significantly to the cost function, our model is steered toward paying more attention to how the features cause minority observations. Scikit-learn makes it easy to do this weighting with `class_weight` parameters (see the [logistic regression doc](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), for example). To obtain `balanced` class weights, we would find the ratio between majority and minority class counts, assigning a higher weight to the minority class by a factor of that ratio.  

As mentioned above, another natural choice is to replicate minority class observations when training to  

